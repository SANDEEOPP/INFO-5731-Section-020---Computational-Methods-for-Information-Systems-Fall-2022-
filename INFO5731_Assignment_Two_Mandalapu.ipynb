{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [Apple iPhone 11](https://www.amazon.com/Apple-iPhone-11-64GB-Unlocked/dp/B07ZPKF8RG/ref=sr_1_13?dchild=1&keywords=iphone+12&qid=1631721363&sr=8-13) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of the film [Shang-Chi and the Legend of the Ten Rings](https://www.imdb.com/title/tt9376612/reviews?ref_=tt_sa_3) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 100 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 tweets by using hashtag [\"#blacklivesmatter\"](https://twitter.com/hashtag/blacklivesmatter) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "main_text = [] # List to store Review headings\n",
        "sub_text =[] #List to store reviews\n",
        "\n",
        "#Collected the IMDB ratings Running shoew review page link\n",
        "for number in range(100):\n",
        "  requested_link = \"https://www.imdb.com/title/tt9376612/reviews?ref_=tt_sa_3\" + str(number) # Generating link dynamically\n",
        "  #Getting Request for the link above\n",
        "  web_page = requests.get(requested_link)\n",
        "  soup = BeautifulSoup(web_page.text, 'html.parser')\n",
        "  rating_reviews = soup.find_all(class_='rating-other-user-rating')\n",
        "  text_reviews = soup.find_all(class_='title')\n",
        "  for ele, sub_ele in zip(rating_reviews, text_reviews) : # Iterating through the list\n",
        "      main_text.append(ele.text) #Appending to empty list\n",
        "      sub_text.append(sub_ele.text)\n",
        "\n",
        "#Dataframe Creation by combining collected data and making the column names of our own!\n",
        "IMDB_rating = pd.DataFrame(list(zip(main_text, sub_text)), columns =['Rating', 'Glimpse of Review']) \n",
        "print(\"Obtained Data Frame Length is {0}\".format(len(IMDB_rating)))\n",
        "IMDB_rating"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "Me-uQETGcCuA",
        "outputId": "b437c16e-cb24-4cf2-babd-6685c7af289a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtained Data Frame Length is 2500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  Rating                                  Glimpse of Review\n",
              "0     \\n\\n\\n\\n\\n\\n5/10\\n                           Sort of disappointed..\\n\n",
              "1     \\n\\n\\n\\n\\n\\n5/10\\n   Questionable script (and Awkwafina is just th...\n",
              "2     \\n\\n\\n\\n\\n\\n6/10\\n   Marks a bland start to the new phase of Marve...\n",
              "3     \\n\\n\\n\\n\\n\\n7/10\\n   This could've been great if it wasn't for the...\n",
              "4     \\n\\n\\n\\n\\n\\n8/10\\n   beautiful science fiction, fantasy and action...\n",
              "...                  ...                                                ...\n",
              "2495  \\n\\n\\n\\n\\n\\n8/10\\n                              Simu Liu Brought It\\n\n",
              "2496  \\n\\n\\n\\n\\n\\n7/10\\n                        house of mouse and marvel\\n\n",
              "2497  \\n\\n\\n\\n\\n\\n7/10\\n                                   It was okay...\\n\n",
              "2498  \\n\\n\\n\\n\\n\\n7/10\\n              Solid first half, messy second half\\n\n",
              "2499  \\n\\n\\n\\n\\n\\n4/10\\n   Soulless movie, even a soul sucking monster c...\n",
              "\n",
              "[2500 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a63bf78-1002-4bed-a66f-e337feee659c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rating</th>\n",
              "      <th>Glimpse of Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n5/10\\n</td>\n",
              "      <td>Sort of disappointed..\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n5/10\\n</td>\n",
              "      <td>Questionable script (and Awkwafina is just th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n6/10\\n</td>\n",
              "      <td>Marks a bland start to the new phase of Marve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n7/10\\n</td>\n",
              "      <td>This could've been great if it wasn't for the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n8/10\\n</td>\n",
              "      <td>beautiful science fiction, fantasy and action...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2495</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n8/10\\n</td>\n",
              "      <td>Simu Liu Brought It\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2496</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n7/10\\n</td>\n",
              "      <td>house of mouse and marvel\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n7/10\\n</td>\n",
              "      <td>It was okay...\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2498</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n7/10\\n</td>\n",
              "      <td>Solid first half, messy second half\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>\\n\\n\\n\\n\\n\\n4/10\\n</td>\n",
              "      <td>Soulless movie, even a soul sucking monster c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2500 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a63bf78-1002-4bed-a66f-e337feee659c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2a63bf78-1002-4bed-a66f-e337feee659c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2a63bf78-1002-4bed-a66f-e337feee659c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMDB_rating.to_csv('IMDB.csv')"
      ],
      "metadata": {
        "id": "8eeUoNfniGlu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vATjQNTY8buA",
        "outputId": "6001cb8a-3f6a-48a9-fa12-fec851e82d81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# nltk.download('data')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import numpy as np\n",
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_frame=IMDB_rating\n",
        "data_frame=data_frame[data_frame[\"Glimpse of Review\"]!= \"\"] #Removing null review\n",
        "print(data_frame)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waasO-hobn8r",
        "outputId": "ddd0d3b4-540d-4e9d-cfb6-6cdaa1ac10f4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Rating                                  Glimpse of Review\n",
            "0     \\n\\n\\n\\n\\n\\n5/10\\n                           Sort of disappointed..\\n\n",
            "1     \\n\\n\\n\\n\\n\\n5/10\\n   Questionable script (and Awkwafina is just th...\n",
            "2     \\n\\n\\n\\n\\n\\n6/10\\n   Marks a bland start to the new phase of Marve...\n",
            "3     \\n\\n\\n\\n\\n\\n7/10\\n   This could've been great if it wasn't for the...\n",
            "4     \\n\\n\\n\\n\\n\\n8/10\\n   beautiful science fiction, fantasy and action...\n",
            "...                  ...                                                ...\n",
            "2495  \\n\\n\\n\\n\\n\\n8/10\\n                              Simu Liu Brought It\\n\n",
            "2496  \\n\\n\\n\\n\\n\\n7/10\\n                        house of mouse and marvel\\n\n",
            "2497  \\n\\n\\n\\n\\n\\n7/10\\n                                   It was okay...\\n\n",
            "2498  \\n\\n\\n\\n\\n\\n7/10\\n              Solid first half, messy second half\\n\n",
            "2499  \\n\\n\\n\\n\\n\\n4/10\\n   Soulless movie, even a soul sucking monster c...\n",
            "\n",
            "[2500 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')\n",
        "stopword_list = stopwords.words('english')\n",
        "snow_stemmer = SnowballStemmer(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean(df):\n",
        "    data_frame.loc[:,[\"clean_txt\"]]=data_frame[\"Glimpse of Review\"].apply(lambda ele: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", ele))\n",
        "    data_frame.loc[:,[\"clean_txt\"]]=data_frame[\"clean_txt\"].apply(lambda ele: re.sub(r\"\\d+\", \"\", ele))\n",
        "    data_frame.loc[:,[\"clean_txt\"]]=data_frame[\"clean_txt\"].apply(lambda string: ' '.join([w for w in string.split() if w not in (stopword_list)]))\n",
        "    data_frame.loc[:,[\"clean_txt\"]]=data_frame[\"clean_txt\"].str.lower()\n",
        "    data_frame.loc[:,[\"tokens\"]] = data_frame[\"clean_txt\"].apply(lambda x: word_tokenize(x))\n",
        "    def word_stemmer(text):\n",
        "      stem_text = [snow_stemmer.stem(i) for i in text]\n",
        "      return stem_text\n",
        "    data_frame.loc[:,['text_stem']] = data_frame[\"tokens\"].apply(lambda x: word_stemmer(x))\n",
        "    def word_lemmatizer(text):\n",
        "      lem_text = [lemmatizer.lemmatize(i) for i in text]\n",
        "      return lem_text\n",
        "    data_frame.loc[:,['text_lem']] = data_frame[\"tokens\"].apply(lambda x: word_lemmatizer(x))\n",
        "    return data_frame\n",
        "c = clean(data_frame)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P16PMhR7KrD",
        "outputId": "016a44a5-6de3-4d0c-9735-b4bcf6462f96"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Rating                                  Glimpse of Review  \\\n",
            "0     \\n\\n\\n\\n\\n\\n5/10\\n                           Sort of disappointed..\\n   \n",
            "1     \\n\\n\\n\\n\\n\\n5/10\\n   Questionable script (and Awkwafina is just th...   \n",
            "2     \\n\\n\\n\\n\\n\\n6/10\\n   Marks a bland start to the new phase of Marve...   \n",
            "3     \\n\\n\\n\\n\\n\\n7/10\\n   This could've been great if it wasn't for the...   \n",
            "4     \\n\\n\\n\\n\\n\\n8/10\\n   beautiful science fiction, fantasy and action...   \n",
            "...                  ...                                                ...   \n",
            "2495  \\n\\n\\n\\n\\n\\n8/10\\n                              Simu Liu Brought It\\n   \n",
            "2496  \\n\\n\\n\\n\\n\\n7/10\\n                        house of mouse and marvel\\n   \n",
            "2497  \\n\\n\\n\\n\\n\\n7/10\\n                                   It was okay...\\n   \n",
            "2498  \\n\\n\\n\\n\\n\\n7/10\\n              Solid first half, messy second half\\n   \n",
            "2499  \\n\\n\\n\\n\\n\\n4/10\\n   Soulless movie, even a soul sucking monster c...   \n",
            "\n",
            "                                              clean_txt  \\\n",
            "0                                     sort disappointed   \n",
            "1                   questionable script awkwafina worst   \n",
            "2     marks bland start new phase marvel cinematic u...   \n",
            "3               this couldve great wasnt cheesy writing   \n",
            "4        beautiful science fiction fantasy action movie   \n",
            "...                                                 ...   \n",
            "2495                                simu liu brought it   \n",
            "2496                                 house mouse marvel   \n",
            "2497                                            it okay   \n",
            "2498                 solid first half messy second half   \n",
            "2499  soulless movie even soul sucking monster could...   \n",
            "\n",
            "                                                 tokens  \\\n",
            "0                                  [sort, disappointed]   \n",
            "1              [questionable, script, awkwafina, worst]   \n",
            "2     [marks, bland, start, new, phase, marvel, cine...   \n",
            "3        [this, couldve, great, wasnt, cheesy, writing]   \n",
            "4     [beautiful, science, fiction, fantasy, action,...   \n",
            "...                                                 ...   \n",
            "2495                           [simu, liu, brought, it]   \n",
            "2496                             [house, mouse, marvel]   \n",
            "2497                                         [it, okay]   \n",
            "2498          [solid, first, half, messy, second, half]   \n",
            "2499  [soulless, movie, even, soul, sucking, monster...   \n",
            "\n",
            "                                              text_stem  \\\n",
            "0                                    [sort, disappoint]   \n",
            "1                  [question, script, awkwafina, worst]   \n",
            "2     [mark, bland, start, new, phase, marvel, cinem...   \n",
            "3           [this, couldv, great, wasnt, cheesi, write]   \n",
            "4      [beauti, scienc, fiction, fantasi, action, movi]   \n",
            "...                                                 ...   \n",
            "2495                           [simu, liu, brought, it]   \n",
            "2496                               [hous, mous, marvel]   \n",
            "2497                                         [it, okay]   \n",
            "2498          [solid, first, half, messi, second, half]   \n",
            "2499  [soulless, movi, even, soul, suck, monster, co...   \n",
            "\n",
            "                                               text_lem  \n",
            "0                                  [sort, disappointed]  \n",
            "1              [questionable, script, awkwafina, worst]  \n",
            "2     [mark, bland, start, new, phase, marvel, cinem...  \n",
            "3        [this, couldve, great, wasnt, cheesy, writing]  \n",
            "4     [beautiful, science, fiction, fantasy, action,...  \n",
            "...                                                 ...  \n",
            "2495                           [simu, liu, brought, it]  \n",
            "2496                             [house, mouse, marvel]  \n",
            "2497                                         [it, okay]  \n",
            "2498          [solid, first, half, messy, second, half]  \n",
            "2499  [soulless, movie, even, soul, sucking, monster...  \n",
            "\n",
            "[2500 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQKnPjPDHJHr",
        "outputId": "df98235f-beff-4c8c-81c5-7356cd01441a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting benepar\n",
            "  Downloading benepar-0.2.0.tar.gz (33 kB)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.7/dist-packages (from benepar) (3.7)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.7/dist-packages (from benepar) (3.4.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from benepar) (1.12.1+cu113)\n",
            "Collecting torch-struct>=0.5\n",
            "  Downloading torch_struct-0.5-py3-none-any.whl (34 kB)\n",
            "Collecting tokenizers>=0.9.4\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.6 MB 8.4 MB/s \n",
            "\u001b[?25hCollecting transformers[tokenizers,torch]>=4.2.2\n",
            "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.9 MB 40.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from benepar) (3.17.3)\n",
            "Collecting sentencepiece>=0.1.91\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3 MB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (7.1.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (3.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (21.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (8.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.4.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (4.1.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.6.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (3.0.10)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.9.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.23.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=2.0.9->benepar) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=2.0.9->benepar) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.0.9->benepar) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.9->benepar) (0.0.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.9->benepar) (0.7.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (5.0.0)\n",
            "Collecting tokenizers>=0.9.4\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.6 MB 41.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163 kB 63.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=2.0.9->benepar) (2.0.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->benepar) (1.15.0)\n",
            "Building wheels for collected packages: benepar\n",
            "  Building wheel for benepar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for benepar: filename=benepar-0.2.0-py3-none-any.whl size=37647 sha256=a7c664cc3159aab8d2ee8ff73cde69fbe8665081d23cd0ee7a8ebf84c5209b56\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/6f/a3/4d27ce92766bdedd2cbbbedb8857fb7a53534331191cda4994\n",
            "Successfully built benepar\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, torch-struct, sentencepiece, benepar\n",
            "Successfully installed benepar-0.2.0 huggingface-hub-0.10.0 sentencepiece-0.1.97 tokenizers-0.12.1 torch-struct-0.5 transformers-4.22.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading benepar_en2: Package 'benepar_en2' not found\n",
            "[nltk_data]     in index\n",
            "/usr/local/lib/python3.7/dist-packages/benepar/spacy_plugin.py:12: FutureWarning: BeneparComponent and NonConstituentException have been moved to the benepar module. Use `from benepar import BeneparComponent, NonConstituentException` instead of benepar.spacy_plugin. The benepar.spacy_plugin namespace is deprecated and will be removed in a future version.\n",
            "  FutureWarning,\n",
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping models/benepar_en3.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "%tensorflow_version 2.x\n",
        "!pip install benepar\n",
        "import benepar\n",
        "benepar.download('benepar_en2')\n",
        "from benepar.spacy_plugin import BeneparComponent\n",
        "benepar.download('benepar_en3')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def syn_struc(text):\n",
        "  for t in text:\n",
        "    for token in nlp(t):\n",
        "      return [token.text, '-',token.pos_,'-',token.tag_]\n",
        "data_frame['pos']=data_frame['tokens'].apply(lambda x: syn_struc(x))\n",
        "print(data_frame['pos'].head())\n",
        "print(len(data_frame['pos']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uudGTgVJC-dB",
        "outputId": "03bd7222-d88e-4839-a64e-450e23d238fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0            [sort, -, ADV, -, RB]\n",
            "1    [questionable, -, ADJ, -, JJ]\n",
            "2         [marks, -, NOUN, -, NNS]\n",
            "3           [this, -, PRON, -, DT]\n",
            "4       [beautiful, -, ADJ, -, JJ]\n",
            "Name: pos, dtype: object\n",
            "2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The constituency parsing tree depends on the formalism of setting free syntaxes. In this kind of tree, the sentence is separated into constituents, or at least, sub-states that have a place with a particular classification in the language.A constituency parse tree always contains the words of the sentence as its terminal nodes. \n",
        "Usually, each word has a parent node containing its part-of-speech tag (noun, adjective, verb, etcâ€¦), although this may be omitted in other graphical representations."
      ],
      "metadata": {
        "id": "iwO9DAWrFHo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dependency parsing doesn't utilize phrasal constituents or sub-phrases. All things being equal, the linguistic structure of the sentence is communicated regarding conditions between words â€” that is, coordinated, composed edges between words in a graph.Dependency parsing can be more useful for several downstream tasks like Information Extraction or Question Answering."
      ],
      "metadata": {
        "id": "PEzhbtNSFhwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Entity Recognization\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "for i in range(0,99):\n",
        "    doc = nlp(data_frame['Glimpse of Review'][i])\n",
        "    print([(X.text, X.label_) for X in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmZC1gELLUYx",
        "outputId": "049d23af-063b-4ae3-9c7f-f9306fb70e25"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Sort', 'GPE')]\n",
            "[('Awkwafina', 'GPE')]\n",
            "[('Marks', 'ORG'), ('Marvel Cinematic', 'ORG')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('Mediocre', 'GPE'), ('Marvel', 'GPE')]\n",
            "[]\n",
            "[]\n",
            "[('Hidden Superhero', 'PERSON')]\n",
            "[('4', 'CARDINAL')]\n",
            "[]\n",
            "[('Ten Years', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('MCU', 'ORG')]\n",
            "[('Simu Liu', 'PERSON')]\n",
            "[]\n",
            "[]\n",
            "[('first', 'ORDINAL'), ('half', 'CARDINAL'), ('second', 'ORDINAL'), ('half', 'CARDINAL')]\n",
            "[]\n",
            "[('Sort', 'GPE')]\n",
            "[('Awkwafina', 'GPE')]\n",
            "[('Marks', 'ORG'), ('Marvel Cinematic', 'ORG')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('Mediocre', 'GPE'), ('Marvel', 'GPE')]\n",
            "[]\n",
            "[]\n",
            "[('Hidden Superhero', 'PERSON')]\n",
            "[('4', 'CARDINAL')]\n",
            "[]\n",
            "[('Ten Years', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('MCU', 'ORG')]\n",
            "[('Simu Liu', 'PERSON')]\n",
            "[]\n",
            "[]\n",
            "[('first', 'ORDINAL'), ('half', 'CARDINAL'), ('second', 'ORDINAL'), ('half', 'CARDINAL')]\n",
            "[]\n",
            "[('Sort', 'GPE')]\n",
            "[('Awkwafina', 'GPE')]\n",
            "[('Marks', 'ORG'), ('Marvel Cinematic', 'ORG')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('Mediocre', 'GPE'), ('Marvel', 'GPE')]\n",
            "[]\n",
            "[]\n",
            "[('Hidden Superhero', 'PERSON')]\n",
            "[('4', 'CARDINAL')]\n",
            "[]\n",
            "[('Ten Years', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('MCU', 'ORG')]\n",
            "[('Simu Liu', 'PERSON')]\n",
            "[]\n",
            "[]\n",
            "[('first', 'ORDINAL'), ('half', 'CARDINAL'), ('second', 'ORDINAL'), ('half', 'CARDINAL')]\n",
            "[]\n",
            "[('Sort', 'GPE')]\n",
            "[('Awkwafina', 'GPE')]\n",
            "[('Marks', 'ORG'), ('Marvel Cinematic', 'ORG')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[('Mediocre', 'GPE'), ('Marvel', 'GPE')]\n",
            "[]\n",
            "[]\n",
            "[('Hidden Superhero', 'PERSON')]\n",
            "[('4', 'CARDINAL')]\n",
            "[]\n",
            "[('Ten Years', 'DATE')]\n",
            "[]\n",
            "[]\n",
            "[('MCU', 'ORG')]\n",
            "[('Simu Liu', 'PERSON')]\n",
            "[]\n",
            "[]\n",
            "[('first', 'ORDINAL'), ('half', 'CARDINAL'), ('second', 'ORDINAL'), ('half', 'CARDINAL')]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}